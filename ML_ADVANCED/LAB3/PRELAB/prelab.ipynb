{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prelab :  Discriminative and Generative Models                             By : Ethan Smadja , Tom Urban, Marine Belet \n",
    " Professor : Jae Yun JUN KIM \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Naive Bayes\n",
    " Sources: Scikit-learn\n",
    " 2.1 Example 1: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided script demonstrates the usage of the Bernoulli Naive Bayes classifier, which is typically used for binary feature classification problems.\n",
    "\n",
    "Here’s what the script does at a high level:\n",
    "\n",
    "Dataset Creation:\n",
    "\n",
    "Randomly generates a small dataset (X) with 6 samples and 100 binary features each (features can only have values 0 or 1).\n",
    "Assigns labels y to these samples as [1, 2, 3, 4, 4, 5].\n",
    "Training:\n",
    "\n",
    "Fits a Bernoulli Naive Bayes classifier to this data, learning to associate patterns of binary features with each label.\n",
    "Prediction:\n",
    "\n",
    "Makes predictions on the exact same samples used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[4]\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Generating a random binary dataset\n",
    "X = np.random.randint(2, size=(6, 100))\n",
    "\n",
    "# Labels for each of the 6 samples\n",
    "y = np.array([1, 2, 3, 4, 4, 5])\n",
    "\n",
    "# Creating the Bernoulli Naive Bayes classifier\n",
    "clf = BernoulliNB()\n",
    "\n",
    "# Training the classifier with the dataset\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predicting labels using the trained classifier\n",
    "for i in range(0, 6):\n",
    "    prediction = clf.predict(X[i:(i+1)])\n",
    "    print(clf.predict(X[i:(i+1)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Results:\n",
    "\n",
    "The classifier predicts exactly the labels it was trained on ([1, 2, 3, 4, 4, 5]) because the predictions were performed on the training data itself. This indicates the classifier has successfully memorized these training examples.\n",
    "In practice, predictions on unseen data might differ, highlighting the importance of evaluating performance on separate test sets to gauge true predictive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Example2: MultinomialNaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses a Multinomial Naive Bayes classifier suited for features representing discrete counts (e.g., text frequencies, occurrences of words).\n",
    "\n",
    "It generates random data (X) containing integer counts between 0 and 4 for each feature.\n",
    "Labels (y) from 1 to 6 are assigned to the six samples.\n",
    "The model is trained and immediately tested on the training set itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Generate a random dataset with integer features between 0 and 4\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "\n",
    "# Labels assigned to the 6 samples\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create and train the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict labels for each sample in the training data\n",
    "for i in range(6):\n",
    "    prediction = clf.predict(X[i:i+1])\n",
    "    print(clf.predict(X[i:i+1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Interpretation:\n",
    "\n",
    "Predictions exactly match the true labels because the predictions are made directly on the training set.\n",
    "Such results demonstrate that the classifier can perfectly memorize the small dataset but does not reflect its actual performance on unseen data.\n",
    "In real scenarios, separate testing data is necessary to evaluate model accuracy and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Example 3 : GaussianNaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation:\n",
    "\n",
    "A small dataset (X) of 6 samples is created, separated clearly into two classes (y = [1, 1, 1, 2, 2, 2]).\n",
    "Each class has distinct numeric features that allow the model to distinguish between them clearly.\n",
    "Model Training:\n",
    "\n",
    "Two Gaussian Naive Bayes classifiers (clf and clf_pf) are created and trained:\n",
    "clf.fit(X, y) trains the model on all data at once.\n",
    "clf_pf.partial_fit(X, y, np.unique(y)) demonstrates incremental learning, useful if the dataset is large or streamed.\n",
    "Prediction:\n",
    "\n",
    "Both models predict the class of a new sample [-1, -0.8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (fit): [1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Creating a small dataset with continuous numerical features\n",
    "X = np.array([[-1, -1], [ -1, -2], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "\n",
    "# Labels indicating two classes: class 1 and class 2\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Initialize and train Gaussian Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict class for a new sample [-1, -0.8]\n",
    "print(\"Prediction (fit):\", clf.predict([[-1, -0.8]]))\n",
    "\n",
    "# Initialize GaussianNB classifier with incremental learning (partial_fit)\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, y, np.unique(y))\n",
    "\n",
    "# Predict class for the same sample using partial_fit-trained classifier\n",
    "print(clf_pf.predict([[-1,-0.8]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes classifier makes predictions based on proximity to learned Gaussian distributions (mean and variance for each class).\n",
    "The tested sample was correctly identified as class 1, reflecting accurate learning by both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.4 Example 4 : Filteringspamemails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[129   1]\n",
      " [  9 121]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 27 22:53:50 2017\n",
    "@author: Abhijeet Singh\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to create a dictionary from email text data\n",
    "\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open(mail, encoding='latin1') as m:\n",
    "            for i, line in enumerate(m):\n",
    "                if i == 2:  # Usually, the 3rd line contains useful content\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "\n",
    "    dictionary = Counter(all_words)\n",
    "\n",
    "    # Removing non-alphabetic words and single-character words\n",
    "    for item in list(dictionary):\n",
    "        if not item.isalpha() or len(item) == 1:\n",
    "            del dictionary[item]\n",
    "\n",
    "    # Keeping only the top 3000 most common words\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n",
    "\n",
    "# Function to extract features from emails based on the created dictionary\n",
    "\n",
    "def extract_features(mail_dir, dictionary):\n",
    "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files), 3000))\n",
    "\n",
    "    for docID, fil in enumerate(files):\n",
    "        with open(fil, encoding='latin1') as fi:\n",
    "            for i, line in enumerate(fi):\n",
    "                if i == 2:  # Usually the subject/content line\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        for wordID, d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                features_matrix[docID, wordID] = words.count(word)\n",
    "\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "# Directories for training and testing data\n",
    "train_dir = 'ling-spam/ling-spam/train-mails'\n",
    "test_dir = 'ling-spam/ling-spam/test-mails'\n",
    "\n",
    "# Create dictionary from training data\n",
    "dictionary = make_Dictionary(train_dir)\n",
    "\n",
    "# Create labels for training data (702 mails: first half non-spam, second half spam)\n",
    "train_labels = np.zeros(702)\n",
    "train_labels[351:701] = 1  # Marking spam mails\n",
    "\n",
    "# Extract features from training mails\n",
    "train_matrix = extract_features(train_dir, dictionary)\n",
    "\n",
    "# Train Multinomial Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(train_matrix, train_labels)\n",
    "\n",
    "# Prepare feature vectors for test data\n",
    "test_matrix = extract_features(test_dir, dictionary)\n",
    "\n",
    "# Labels for testing data (first 130 ham, next 130 spam)\n",
    "test_labels = np.zeros(260)\n",
    "test_labels[130:260] = 1\n",
    "\n",
    "# Make predictions on test data and display confusion matrix\n",
    "result = model.predict(test_matrix)\n",
    "\n",
    "# Output confusion matrix to evaluate performance\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "True Negatives (129):\n",
    "Correctly classified ham (non-spam) emails. This indicates the classifier is highly accurate at recognizing legitimate emails.\n",
    "\n",
    "False Positives (1):\n",
    "One ham email was incorrectly identified as spam. This type of error is minor here, but still undesirable since legitimate emails could be missed by the recipient.\n",
    "\n",
    "False Negatives (9):\n",
    "Nine actual spam emails were incorrectly classified as ham. These represent spam emails slipping through the filter.\n",
    "\n",
    "True Positives (121):\n",
    "Correctly classified spam emails. Indicates good spam identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance Insights:\n",
    "The classifier is overall very effective.\n",
    "Accuracy is high:\n",
    "Accuracy ≈96.15%\n",
    "The small number of false positives (1) is good, as users prefer to avoid losing legitimate emails to spam filters.\n",
    "The classifier is slightly less efficient at detecting every spam (9 false negatives), meaning some spam emails might reach inboxes, but performance is still strong overall.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
